{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sometimes this does not hide the warnings, we may need to execute this multiple times.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from sys import platform\n",
    "if platform == \"darwin\":\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise states to be tracked\n",
    "# select any 4 Q-values\n",
    "tracked_values = [((0, 12, 1), (1, 4)), ((1, 12, 4), (1, 2)),\n",
    "                  ((2, 14, 2), (2, 3)), ((3, 18, 5), (3, 4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state-action and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def convert_to_data_frame(model_name, data):\n",
    "    df = pd.DataFrame.from_records(data)\n",
    "    df['name'] = model_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "hp_learning_rate = [0.001, 0.002]\n",
    "hp_gamma = [0.95]\n",
    "decay_rate = [0.0003, 0.0005]\n",
    "layers = [[32, 32], [64, 64], [128, 128]]\n",
    "\n",
    "hp_params = list(product(hp_learning_rate, hp_gamma, decay_rate, layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tracked_values(states_tracks, big=False):\n",
    "    if big:\n",
    "        row = 2\n",
    "        column = 2\n",
    "        figsize = (16, 12)\n",
    "    else:\n",
    "        row = 1\n",
    "        column = 4\n",
    "        figsize = (16, 4)\n",
    "\n",
    "    fig, axs = plt.subplots(row, column, figsize=figsize)\n",
    "    for index, tracked_value in enumerate(tracked_values):\n",
    "        xaxis = np.asarray(\n",
    "            range(0, len(states_tracks[tracked_value[0]][tracked_value[1]])))\n",
    "        plt.subplot(row, column, index + 1)\n",
    "        plt.plot(xaxis,\n",
    "                 np.asarray(states_tracks[tracked_value[0]][tracked_value[1]]))\n",
    "        plt.title(\"({},{},{})\".format(tracked_value[0][0], tracked_value[0][1],\n",
    "                                      tracked_value[0][2]) + \"_\" +\n",
    "                  \"({},{})\".format(tracked_value[1][0], tracked_value[1][1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentParam:\n",
    "    def __init__(self, params):\n",
    "        self.learning_rate = params.get(\"learning_rate\", 0.01)\n",
    "        self.action_size = params[\"action_size\"]\n",
    "        self.state_size = params[\"state_size\"]\n",
    "        self.action_space = params[\"action_space\"]\n",
    "        self.epsilon_max = params.get(\"epsilon_max\", 1.0)\n",
    "        self.epsilon_min = params.get(\"epsilon_min\", 0.0001)\n",
    "        self.epsilon_decay = params.get(\"epsilon_decay\", 0.0009)\n",
    "        self.discount_factor = params.get(\"discount_factor\", 0.95)\n",
    "        self.batch_size = params.get(\"batch_size\", 32)\n",
    "        self.layers = params[\"layers\"]\n",
    "\n",
    "    def _print(self):\n",
    "        attrs = vars(self)\n",
    "        print(\"Agent Params: \")\n",
    "        print(\"********************************************************\")\n",
    "        for key, value in attrs.items():\n",
    "            print(key, '->', value)\n",
    "        print(\"********************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, epsilon_max, epsilon_min, epsilon_decay):\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "    def get_epsilon(self, episode):\n",
    "        epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min\n",
    "                                      ) * np.exp(-self.epsilon_decay * episode)\n",
    "        return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, batch_size, buffer_size=2000):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "\n",
    "    def has_enough_experience(self):\n",
    "        return self.length() > self.batch_size\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, agent_param, state_encoder):\n",
    "        agent_param._print()\n",
    "        # Define size of state and action\n",
    "        self.state_size = agent_param.state_size\n",
    "        self.action_size = agent_param.action_size\n",
    "        self.action_space = agent_param.action_space\n",
    "\n",
    "        self.layers = agent_param.layers\n",
    "\n",
    "        self.discount_factor = agent_param.discount_factor\n",
    "        self.learning_rate = agent_param.learning_rate\n",
    "        self.epsilon_greedy_policy = EpsilonGreedyPolicy(\n",
    "            epsilon_max=agent_param.epsilon_max,\n",
    "            epsilon_min=agent_param.epsilon_min,\n",
    "            epsilon_decay=agent_param.epsilon_decay)\n",
    "        self.batch_size = agent_param.batch_size\n",
    "        self.memory = ReplayBuffer(batch_size=self.batch_size)\n",
    "        self.epsilon = 1\n",
    "\n",
    "        # create main model and target model\n",
    "        self.states_track = collections.defaultdict(dict)\n",
    "        self.initialise_tracking_states()\n",
    "        self.state_encoder = state_encoder\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets\n",
    "        for index, layer_dim in enumerate(self.layers):\n",
    "            if index == 0:\n",
    "                model.add(\n",
    "                    Dense(layer_dim,\n",
    "                          input_dim=self.state_size,\n",
    "                          activation='relu',\n",
    "                          kernel_initializer='he_uniform'))\n",
    "            else:\n",
    "                model.add(\n",
    "                    Dense(layer_dim,\n",
    "                          activation='relu',\n",
    "                          kernel_initializer='he_uniform'))\n",
    "\n",
    "        model.add(\n",
    "            Dense(self.action_size,\n",
    "                  activation='linear',\n",
    "                  kernel_initializer='he_uniform'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Nadam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state, episode, requests):\n",
    "        # Write your code here:\n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in Îµ after we generate each sample from the environment\n",
    "        self.epsilon = self.epsilon_greedy_policy.get_epsilon(episode)\n",
    "        z = np.random.random()\n",
    "        state_actions_index, state_actions = requests\n",
    "        if z > self.epsilon:\n",
    "            # Exploitation: this gets the action corresponding to max q-value of current state\n",
    "            state_encode = self.state_encoder(state, self.state_size)\n",
    "            q_value = self.model.predict(state_encode)\n",
    "            q_value = q_value[0][state_actions_index]\n",
    "\n",
    "            action_index = np.argmax(q_value)\n",
    "            action = state_actions[action_index]\n",
    "        else:\n",
    "            action = self.action_space[np.random.choice(state_actions_index)]\n",
    "        return action\n",
    "\n",
    "    def save_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_model(self):\n",
    "        if not self.memory.has_enough_experience():\n",
    "            return\n",
    "\n",
    "        # Sample batch from the memory\n",
    "        mini_batch = self.memory.sample()\n",
    "        update_input = np.zeros((self.batch_size, self.state_size))\n",
    "        update_target = np.zeros((self.batch_size, self.state_size))\n",
    "        actions, rewards, done = [], [], []\n",
    "\n",
    "        # Set the values of input, action, reward, target and done using memory\n",
    "        # Note the order of <s,a,r,s',done>\n",
    "        for index, experience in enumerate(mini_batch):\n",
    "            state, action, reward, next_state, is_finished = experience\n",
    "\n",
    "            update_input[index] = self.state_encoder(state,\n",
    "                                                     self.state_size,\n",
    "                                                     reshape=False)\n",
    "            update_target[index] = self.state_encoder(next_state,\n",
    "                                                      self.state_size,\n",
    "                                                      reshape=False)\n",
    "\n",
    "            actions.append(self.action_space.index(action))\n",
    "            rewards.append(reward)\n",
    "            done.append(is_finished)\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "\n",
    "        # get your target Q-value on the basis of terminal state\n",
    "        for index in range(self.batch_size):\n",
    "            if done[index]:\n",
    "                # print(True)\n",
    "                target[index][actions[index]] = rewards[index]\n",
    "\n",
    "            else:\n",
    "                target[index][\n",
    "                    actions[index]] = rewards[index] + self.discount_factor * (\n",
    "                        np.amax(target_val[index]))\n",
    "\n",
    "        self.model.fit(update_input,\n",
    "                       target,\n",
    "                       batch_size=self.batch_size,\n",
    "                       epochs=1,\n",
    "                       verbose=0)\n",
    "\n",
    "    def memory_length(self):\n",
    "        return self.memory.length()\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def initialise_tracking_states(self):\n",
    "        for q_values in tracked_values:\n",
    "            state = q_values[0]\n",
    "            action = q_values[1]\n",
    "            self.states_track[state][action] = []\n",
    "\n",
    "    def update_tracking_states(self):\n",
    "        for state in self.states_track.keys():\n",
    "            for action in self.states_track[state].keys():\n",
    "                encoded_state = self.state_encoder(state, self.state_size)\n",
    "                prediction = self.model.predict(encoded_state)\n",
    "                action_index = self.action_space.index(list(action))\n",
    "                self.states_track[state][action].append(\n",
    "                    prediction[0][action_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CabDriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_encoder(state, state_size, reshape=True):\n",
    "    state_encode = env.encode_state(state)\n",
    "    if reshape:\n",
    "        state_encode = np.reshape(state_encode, [1, state_size])\n",
    "    return state_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(agent_param,\n",
    "               max_epsilon=1.0,\n",
    "               episodes=50000,\n",
    "               verbose=False,\n",
    "               name='dqn_agent_state_as_input_arch'):\n",
    "\n",
    "    log_threshold = episodes * .001\n",
    "    threshold = episodes * .001\n",
    "    model_threshold = episodes * 0.01\n",
    "\n",
    "    print(\"number of episodes\", episodes)\n",
    "    print(\"log_threshold\", log_threshold)\n",
    "    print(\"threshold\", threshold)\n",
    "    print(\"model_threshold\", model_threshold)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    agent = DQNAgent(agent_param=agent_param, state_encoder=state_encoder)\n",
    "\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        # tracking total rewards, step count\n",
    "        total_reward = 0\n",
    "\n",
    "        state = env.reset()[2]\n",
    "        is_finished = False\n",
    "\n",
    "        current_episode = episode + 1\n",
    "\n",
    "        while not is_finished:\n",
    "\n",
    "            action = agent.get_action(state, episode, env.get_requests(state))\n",
    "\n",
    "            reward = env.get_reward(state, action, Time_matrix)\n",
    "\n",
    "            next_state, is_finished, episode_time_till_now = env.get_next_state(\n",
    "                state, action, Time_matrix)\n",
    "\n",
    "            agent.save_experience(state, action, reward, next_state,\n",
    "                                  is_finished)\n",
    "\n",
    "            # every time step do the training\n",
    "            agent.train_model()\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            # Store the rewards\n",
    "            if is_finished and current_episode % log_threshold == 0:\n",
    "                total_rewards.append(total_reward)\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        \"episode: {}, rewards: {}, replay_buffer_size: {}, episode_time: {}, epsilon: {}\"\n",
    "                        .format(current_episode, total_reward,\n",
    "                                agent.memory_length(), episode_time_till_now,\n",
    "                                agent.epsilon))\n",
    "\n",
    "        if (current_episode % model_threshold) == 0:\n",
    "            print(\"saving model for episode \", current_episode)\n",
    "            agent.save(\"./\" + name + \"_dqn_cab_driver.h5\")\n",
    "\n",
    "        if (current_episode % threshold) == 0:\n",
    "            agent.update_tracking_states()\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"elapsed_time {} minutes\".format(round(elapsed_time / 60, 2)))\n",
    "\n",
    "    return agent, total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stats(df):\n",
    "    global stats\n",
    "    if stats is not None:\n",
    "        stats = pd.concat([stats, df], axis=0)\n",
    "    else:\n",
    "        stats = df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of episodes 15000\n",
      "log_threshold 75.0\n",
      "threshold 75.0\n",
      "model_threshold 750.0\n",
      "Agent Params: \n",
      "********************************************************\n",
      "learning_rate -> 0.001\n",
      "action_size -> 21\n",
      "state_size -> 36\n",
      "action_space -> [[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]]\n",
      "epsilon_max -> 1.0\n",
      "epsilon_min -> 1e-06\n",
      "epsilon_decay -> 0.0005\n",
      "discount_factor -> 0.95\n",
      "batch_size -> 32\n",
      "layers -> [16, 32]\n",
      "********************************************************\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                592       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 21)                693       \n",
      "=================================================================\n",
      "Total params: 1,829\n",
      "Trainable params: 1,829\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/akumar/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-37ed2b239dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                   \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                   name=\"dqn_agent_state_as_input_arch_16_32\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-329f2bf0577d>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(agent_param, max_epsilon, episodes, verbose, name)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# every time step do the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5e3a14293f9a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                        verbose=0)\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmemory_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m     90\u001b[0m            \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_t_batch\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3500\u001b[0m     \"\"\"\n\u001b[1;32m   3501\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3502\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3503\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3408\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3410\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3411\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3522\u001b[0m         \u001b[0mkth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3523\u001b[0m     \u001b[0;31m# Check if the array contains any nan's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3524\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3525\u001b[0m         \u001b[0mkth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPISODES = 15000\n",
    "agent_param_dict = {\n",
    "    \"action_size\": len(env.action_space),\n",
    "    \"state_size\": env.state_size,\n",
    "    \"action_space\": env.action_space,\n",
    "    \"epsilon_max\": 1.0,\n",
    "    \"epsilon_min\": 0.000001,\n",
    "    \"epsilon_decay\": 0.0005,\n",
    "    \"discount_factor\": 0.95,\n",
    "    \"batch_size\": 32,\n",
    "    \"layers\": [16, 32],\n",
    "    \"learning_rate\": 0.001\n",
    "}\n",
    "\n",
    "agent_param = AgentParam(agent_param_dict)\n",
    "\n",
    "agent, total_rewards = q_learning(agent_param=agent_param,\n",
    "                                  episodes=EPISODES,\n",
    "                                  verbose=True,\n",
    "                                  name=\"dqn_agent_state_as_input_arch_16_32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting total rewards\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.plot(list(range(len(total_rewards))), total_rewards)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tracked_values(agent.states_track, big=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_greedy_policy = EpsilonGreedyPolicy(\n",
    "    epsilon_max=agent_param.epsilon_max,\n",
    "    epsilon_min=agent_param.epsilon_min,\n",
    "    epsilon_decay=agent_param.epsilon_decay)\n",
    "\n",
    "time = np.arange(0, EPISODES)\n",
    "epsilon = []\n",
    "for i in range(0, EPISODES):\n",
    "    epsilon.append(epsilon_greedy_policy.get_epsilon(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
