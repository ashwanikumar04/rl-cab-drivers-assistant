{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sometimes this does not hide the warnings, we may need to execute this multiple times.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from sys import platform\n",
    "if platform == \"darwin\":\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(style='dark')\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise states to be tracked\n",
    "# select any 3 Q-values\n",
    "tracked_values = [((0, 12, 1), (1, 4)), \n",
    "                  ((1, 12, 4), (1, 2)),\n",
    "                  ((2, 14, 2), (2, 3))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state-action and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tracked_values(states_tracks):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 2))\n",
    "    for index, tracked_value in enumerate(tracked_values):\n",
    "        xaxis = np.asarray(\n",
    "            range(0, len(states_tracks[tracked_value[0]][tracked_value[1]])))\n",
    "        plt.subplot(1, 3, index + 1)\n",
    "        plt.plot(xaxis,\n",
    "                 np.asarray(states_tracks[tracked_value[0]][tracked_value[1]]))\n",
    "        plt.title(\"({},{},{})\".format(tracked_value[0][0], tracked_value[0][1],\n",
    "                                      tracked_value[0][2]) + \"_\" +\n",
    "                  \"({},{})\".format(tracked_value[1][0], tracked_value[1][1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentParam:\n",
    "    def __init__(self, params):\n",
    "        self.learning_rate = params.get(\"learning_rate\", 0.01)\n",
    "        self.action_size = params[\"action_size\"]\n",
    "        self.state_size = params[\"state_size\"]\n",
    "        self.action_space = params[\"action_space\"]\n",
    "        self.epsilon_max = params.get(\"epsilon_max\", 1)\n",
    "        self.epsilon_min = params.get(\"epsilon_min\", 0.0001)\n",
    "        self.epsilon_decay = params.get(\"epsilon_decay\", 0.0009)\n",
    "        self.discount_factor = params.get(\"discount_factor\", 0.95)\n",
    "        self.batch_size = params.get(\"batch_size\", 32)\n",
    "        self.layers = params[\"layers\"]\n",
    "\n",
    "    def _print(self):\n",
    "        attrs = vars(self)\n",
    "        print(\"Agent Params: \")\n",
    "        print(\"********************************************************\")\n",
    "        for key, value in attrs.items():\n",
    "            print(key, '->', value)\n",
    "        print(\"********************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, epsilon_max, epsilon_min, epsilon_decay):\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "    def get_epsilon(self, episode):\n",
    "        epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min\n",
    "                                      ) * np.exp(-self.epsilon_decay * episode)\n",
    "        return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, batch_size, buffer_size=2000):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "\n",
    "    def has_enough_experience(self):\n",
    "        return self.length() > self.batch_size\n",
    "    \n",
    "    def length(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, agent_param, state_encoder):\n",
    "        agent_param._print()\n",
    "        # Define size of state and action\n",
    "        self.state_size = agent_param.state_size\n",
    "        self.action_size = agent_param.action_size\n",
    "        self.action_space = agent_param.action_space\n",
    "\n",
    "        self.layers = agent_param.layers\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = agent_param.discount_factor\n",
    "        self.learning_rate = agent_param.learning_rate\n",
    "        self.epsilon_greedy_policy = EpsilonGreedyPolicy(\n",
    "            epsilon_max=agent_param.epsilon_max,\n",
    "            epsilon_min=agent_param.epsilon_min,\n",
    "            epsilon_decay=agent_param.epsilon_decay)\n",
    "        self.batch_size = agent_param.batch_size\n",
    "        self.memory = ReplayBuffer(batch_size=self.batch_size)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.states_track = collections.defaultdict(dict)\n",
    "        self.initialise_tracking_states()\n",
    "        self.state_encoder = state_encoder\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets\n",
    "        for index, layer_dim in enumerate(self.layers):\n",
    "            if index == 0:\n",
    "                model.add(\n",
    "                    Dense(layer_dim,\n",
    "                          input_dim=self.state_size,\n",
    "                          activation='relu',\n",
    "                          kernel_initializer='he_uniform'))\n",
    "            else:\n",
    "                model.add(\n",
    "                    Dense(layer_dim,\n",
    "                          activation='relu',\n",
    "                          kernel_initializer='he_uniform'))\n",
    "\n",
    "        model.add(\n",
    "            Dense(self.action_size,\n",
    "                  activation='linear',\n",
    "                  kernel_initializer='he_uniform'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Nadam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state, episode, requests):\n",
    "        # Write your code here:\n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in ε after we generate each sample from the environment\n",
    "        epsilon = self.epsilon_greedy_policy.get_epsilon(episode)\n",
    "        z = np.random.random()\n",
    "        state_actions_index, state_actions = requests\n",
    "        if z > epsilon:\n",
    "            # Exploitation: this gets the action corresponding to max q-value of current state\n",
    "            state_encode = self.state_encoder(state, self.state_size)\n",
    "            q_value = self.model.predict(state_encode)\n",
    "            q_value = q_value[0][state_actions_index]\n",
    "\n",
    "            action_index = np.argmax(q_value)\n",
    "            action = state_actions[action_index]\n",
    "        else:\n",
    "            action = self.action_space[np.random.choice(state_actions_index)]\n",
    "        return action\n",
    "\n",
    "    def append_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_model(self):\n",
    "        if not self.memory.has_enough_experience():\n",
    "            return\n",
    "\n",
    "        # Sample batch from the memory\n",
    "        mini_batch = self.memory.sample()\n",
    "        update_input = np.zeros((self.batch_size, self.state_size))\n",
    "        update_target = np.zeros((self.batch_size, self.state_size))\n",
    "        actions, rewards, done = [], [], []\n",
    "\n",
    "        # Set the values of input, action, reward, target and done using memory\n",
    "        # Note the order of <s,a,r,s',done>\n",
    "        for index, experience in enumerate(mini_batch):\n",
    "            state, action, reward, next_state, is_finished = experience\n",
    "\n",
    "            update_input[index] = env.encode_state(state)\n",
    "            update_target[index] = env.encode_state(next_state)\n",
    "\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            done.append(is_finished)\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "\n",
    "        # get your target Q-value on the basis of terminal state\n",
    "        for index in range(self.batch_size):\n",
    "            if done[index]:\n",
    "                # print(True)\n",
    "                target[index][actions[index]] = rewards[index]\n",
    "\n",
    "            else:\n",
    "                target[index][\n",
    "                    actions[index]] = rewards[index] + self.discount_factor * (\n",
    "                        np.amax(target_val[index]))\n",
    "\n",
    "        self.model.fit(update_input,\n",
    "                       target,\n",
    "                       batch_size=self.batch_size,\n",
    "                       epochs=1,\n",
    "                       verbose=0)\n",
    "\n",
    "    def memory_length(self):\n",
    "        return self.memory.length()\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def initialise_tracking_states(self):\n",
    "        for q_values in tracked_values:\n",
    "            state = q_values[0]\n",
    "            action = q_values[1]\n",
    "            self.states_track[state][action] = []\n",
    "\n",
    "    def update_tracking_states(self):\n",
    "        for state in self.states_track.keys():\n",
    "            for action in self.states_track[state].keys():\n",
    "                encoded_state = self.state_encoder(state, self.state_size)\n",
    "                prediction = self.model.predict(encoded_state)\n",
    "                action_index = self.action_space.index(list(action))\n",
    "                states_track[state][action].append(prediction[0][action_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CabDriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_encoder(state, state_size):\n",
    "    state_encode = env.encode_state(state)\n",
    "    state_encode = np.reshape(state_encode, [1, state_size])\n",
    "    return state_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Params: \n",
      "********************************************************\n",
      "learning_rate -> 0.001\n",
      "action_size -> 21\n",
      "state_size -> 36\n",
      "action_space -> [[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]]\n",
      "epsilon_max -> 1\n",
      "epsilon_min -> 0.0001\n",
      "epsilon_decay -> 0.0009\n",
      "discount_factor -> 0.95\n",
      "batch_size -> 32\n",
      "layers -> [200, 150, 100]\n",
      "********************************************************\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               7400      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 21)                2121      \n",
      "=================================================================\n",
      "Total params: 54,771\n",
      "Trainable params: 54,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent_param_dict = {\n",
    "    \"action_size\": len(env.action_space),\n",
    "    \"state_size\": env.state_size,\n",
    "    \"action_space\": env.action_space,\n",
    "    \"epsilon_max\": 1,\n",
    "    \"epsilon_min\": 0.0001,\n",
    "    \"epsilon_decay\": 0.0009,\n",
    "    \"discount_factor\": 0.95,\n",
    "    \"batch_size\": 32,\n",
    "    \"layers\": [32, 32],\n",
    "    \"learning_rate\": 0.001\n",
    "}\n",
    "\n",
    "agent_param = AgentParam(agent_param_dict)\n",
    "agent = DQNAgent(agent_param=agent_param, state_encoder=state_encoder)\n",
    "\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/akumar/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "episode: 1   score: -178.0   memory length: 162\n",
      "episode: 2   score: -36.0   memory length: 326\n",
      "episode: 3   score: -325.0   memory length: 481\n",
      "episode: 4   score: -153.0   memory length: 629\n",
      "episode: 5   score: -315.0   memory length: 786\n",
      "episode: 6   score: -308.0   memory length: 936\n",
      "episode: 7   score: -133.0   memory length: 1088\n",
      "episode: 8   score: -141.0   memory length: 1246\n",
      "episode: 9   score: -298.0   memory length: 1370\n",
      "episode: 10   score: -109.0   memory length: 1496\n"
     ]
    }
   ],
   "source": [
    "log_threshold = Episodes * .05\n",
    "threshold = Episodes * .001\n",
    "model_threshold = Episodes * 0.05\n",
    "\n",
    "for episode in range(Episodes):\n",
    "\n",
    "    # tracking total rewards, step count\n",
    "    total_reward = 0\n",
    "\n",
    "    state = env.reset()[2]\n",
    "    is_finished = False\n",
    "\n",
    "    current_episode = episode + 1\n",
    "\n",
    "    while not is_finished:\n",
    "\n",
    "        action = agent.get_action(state, episode, env.get_requests(state))\n",
    "\n",
    "        reward = env.get_reward(state, action, Time_matrix)\n",
    "\n",
    "        next_state, is_finished = env.get_next_state(state, action,\n",
    "                                                     Time_matrix)\n",
    "        agent.append_experience(state, action, reward, next_state, is_finished)\n",
    "\n",
    "        # every time step do the training\n",
    "        agent.train_model()\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        # Store the rewards\n",
    "        if is_finished and current_episode % log_threshold == 0:\n",
    "            total_rewards.append(total_reward)\n",
    "            print(\"episode:\", current_episode, \"  score:\", total_reward,\n",
    "                  \"  memory length:\", agent.memory_length())\n",
    "\n",
    "    if (current_episode % model_threshold) == 0:\n",
    "        agent.save(\"./dqn_cab_driver.h5\")\n",
    "\n",
    "    if (current_episode % threshold) == 0:\n",
    "        agent.update_tracking_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {(0, 12, 1): {(1, 4): []}, (1, 12, 4): {(1, 2): []}, (2, 14, 2): {(2, 3): []}})\n"
     ]
    }
   ],
   "source": [
    "print(agent.states_track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x144 with 3 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 157,
       "width": 876
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tracked_values(agent.states_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
